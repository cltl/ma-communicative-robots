{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c045b6f-2966-40da-bbf0-2cca52370a3b",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b8c17-89bd-41d3-82fb-5999f9c08d83",
   "metadata": {},
   "source": [
    "This notebook shows how you can chat with Llama through an EMISSOR client. The EMISSOR layer will capture the interaction as a scenario for further analysis.\n",
    "For chatting with a LLama model, we use the [LangChain layer on top of Ollama](https://api.python.langchain.com/en/latest/chat_models/langchain_ollama.chat_models.ChatOllama.html). \n",
    "\n",
    "Ollama allows you to pull a model from the web to your local machine and use a ```chat``` function to send instructions to local model to get a response. \n",
    "\n",
    "```\n",
    "https://github.com/ollama/ollama\n",
    "```\n",
    "\n",
    "Instead of the Ollama ```chat``` function, we will create a client through ChatOllama so that we can set parameters for the behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b76e5-6d8a-4091-b529-802049c5e39b",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26547e64-b644-44da-9685-592e73d528fb",
   "metadata": {},
   "source": [
    "The following dependencies need to be installed for this notebook. The dependencies are in the requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66457f-d098-46b4-aca3-876400dcc702",
   "metadata": {},
   "source": [
    "* pip install emissor\n",
    "* pip install cltl.combots\n",
    "* pip install ollama\n",
    "* pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e213902-fd4d-4d5a-9d17-8c558af190db",
   "metadata": {},
   "source": [
    "There are various versions of models. We are going to pull the smallest Llama3.2 model that already gives reasonable performance but only works for text input.\n",
    "To be able to access the model through Ollama, we need to pull it from the terminal in the same virtual environment:\n",
    "\n",
    "```\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "\n",
    "You only need to do this once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63864de-b1c2-4051-9a2f-23bc2dbb84ee",
   "metadata": {},
   "source": [
    "### Loading Llama in ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54334a9-b0bc-440d-868d-924a772b9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llama_model = \"llama3.2:1b\" ### 1B\n",
    "#llama_model = \"llama3.2\" ### 3B\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = llama_model,\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e96a772-4705-4291-941e-909658eae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = { 'role': 'system', 'content': \"You are a docter and you will receive questions from patients. Be brief and no more than two sentences.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387c124-7772-4511-9afb-cb83449ca47a",
   "metadata": {},
   "source": [
    "### Creating an EMISSOR client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a29313-9cd6-4321-9f0d-fd2746ae2ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from leolani_client import LeolaniChatClient\n",
    "emissor_path = \"./emissor\"\n",
    "HUMAN=\"Piek\"\n",
    "AGENT=\"Llama\"\n",
    "leolaniClient = LeolaniChatClient(emissor_path=emissor_path, agent=AGENT, human=HUMAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f437945-4b5c-418a-b9a1-55b7435d5da6",
   "metadata": {},
   "source": [
    "### Interaction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa6d0c3-a5fa-4a5a-b2f9-093ee36aba51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a docter and you will receive questions from patients. Be brief and no more than two sentences.'}]\n",
      "Llama: <|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm ready to answer your questions as a doctor. Go ahead and ask away, and I'll respond with concise answers tailored to your specific needs.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I have a cold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: I have a cold\n",
      "Llama: Since you're experiencing symptoms of a cold, it's likely that you've already been vaccinated for flu, which protects against other types of infections. To help alleviate symptoms like congestion, coughing, and fatigue, consider over-the-counter medications such as pain relievers or decongestants to help manage your symptoms.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: bye\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "history.append(instruct)\n",
    "print(history)\n",
    "### First prompt\n",
    "response = llm.invoke(history)\n",
    "utterance = response.content\n",
    "print(AGENT + \": \" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance) \n",
    "prompt = { 'role': 'system', 'content': utterance}\n",
    "history.append(prompt)\n",
    "\n",
    "utterance = input(\"\\n\")\n",
    "print(HUMAN + \": \" + utterance)\n",
    "leolaniClient._add_utterance(HUMAN, utterance)\n",
    "prompt = { 'role': 'user', 'content': utterance}\n",
    "history.append(prompt)\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    response = llm.invoke(history)\n",
    "    utterance = response.content\n",
    "    print(AGENT + \": \" + utterance)\n",
    "    leolaniClient._add_utterance(AGENT, utterance) \n",
    "    prompt = { 'role': 'system', 'content': utterance}\n",
    "    history.append(prompt)\n",
    "\n",
    "    utterance = input(\"\\n\")\n",
    "    print(HUMAN + \": \" + utterance)\n",
    "    leolaniClient._add_utterance(HUMAN, utterance)\n",
    "    prompt = { 'role': 'user', 'content': utterance}\n",
    "    history.append(prompt)\n",
    "\n",
    "##### After completion, we save the scenario in the defined emissor folder.\n",
    "leolaniClient._save_scenario() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1136a7a-2284-46ff-80b3-00247195e022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
