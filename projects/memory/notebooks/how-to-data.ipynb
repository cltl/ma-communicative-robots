{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to data\n",
    "\n",
    "This notebook explains the data structure.\n",
    "\n",
    "Please make sure to get familiarized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    \"\"\"Read json file.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    path: path to the json\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loaded: loaded dict\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as stream:\n",
    "        loaded = json.load(stream)\n",
    "\n",
    "    return loaded\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    copied from https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "\n",
    "    \"\"\"\n",
    "    return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all of the data to `data_all`\n",
    "\n",
    "The filenames are named as `<maximum_history>_<memory_capacity>.json`\n",
    "\n",
    "The `<maximum_history>` refers to the number of observations. For example, if the value \n",
    "of this is 128, it means that the agent has observed the objects and their location 128 times.\n",
    "\n",
    "The value of `<maximum_history>` is fixed to 128 for simplicity.\n",
    "\n",
    "`<memory_capacity>` refers to the maximum number of memories per memory system. \n",
    "For example, if the value of this is 2, it means that the agent has 2 episodic and 2 \n",
    "semantic memories in its brain. Obviously the more memories the agent has, the more likely \n",
    "it can answer given questions.\n",
    "\n",
    "every `json` file has two data splits. One is `val` and the other is `test`. You should try\n",
    "various templates and choose the template that has the best score on the `val` split, and then\n",
    "run the same template on the `test` split. You should report the numbers of the different templates\n",
    "to better compare them. \n",
    "\n",
    "You don't have to merge the files, since they are all different datasets. At this moment, \n",
    "there are 7 json files. In other words, you are reporting validation and test scores on all seven files.\n",
    "\n",
    "All 7 files have the scores from the hand-crafted models. For example, if you read `128_64.json`,\n",
    "you'll see the below key: value pairs:\n",
    "\n",
    "```\n",
    "\"accuracy\": {\n",
    "    \"val\": 0.96875,\n",
    "    \"test\": 0.9921875\n",
    "```\n",
    "\n",
    "The accuracy on both validation and test splits are close to 1.0, which means that it was able\n",
    "to answer most of the questions. This makes sense cuz it's got a lot of memories (i.e., 64 per memory system)\n",
    "\n",
    "Let's also look at the performance of the hand-crafted model on `128_8.json`:\n",
    "\n",
    "```\n",
    "\"accuracy\": {\n",
    "    \"val\": 0.3046875,\n",
    "    \"test\": 0.4375\n",
    "}\n",
    "```\n",
    "As you can see this performed worse than the model that has 64 memories per system.\n",
    "\n",
    "In the below cell, we'll load the data and see actually what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d513fc92164c0c9b2abdfdc4bf3aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/128_1.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 1\n",
      "\n",
      "../data/128_2.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 2\n",
      "\n",
      "../data/128_4.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 4\n",
      "\n",
      "../data/128_8.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 8\n",
      "\n",
      "../data/128_16.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 16\n",
      "\n",
      "../data/128_32.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 32\n",
      "\n",
      "../data/128_64.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_all = {}\n",
    "\n",
    "paths = glob(\"../data/*.json\")\n",
    "paths.sort(key=natural_keys)\n",
    "for path in tqdm(paths):\n",
    "    data = read_json(path)\n",
    "\n",
    "    print(path, \"\\t\", list(data.keys()))\n",
    "    data_all[path] = data\n",
    "\n",
    "    maximum_history = os.path.basename(path).split(\"_\")[0]\n",
    "    memory_capacity = os.path.basename(path).split(\"_\")[1].split(\".json\")[0]\n",
    "\n",
    "    print(\n",
    "        f\"maximum_history: {maximum_history}\\nmemory_capacity per system: {memory_capacity}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at a random data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_point = random.choice(data_all[\"../data/128_4.json\"][\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This has five {key:value} pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct_answer': 'closet',\n",
      " 'episodic_memory_system': [[\"Mary's knife\",\n",
      "                             'AtLocation',\n",
      "                             \"Mary's forest\",\n",
      "                             1004845854.5152943],\n",
      "                            [\"Charles's cat\",\n",
      "                             'AtLocation',\n",
      "                             \"Charles's lap\",\n",
      "                             1004932254.5161054],\n",
      "                            [\"Karen's cow\",\n",
      "                             'AtLocation',\n",
      "                             \"Karen's countryside\",\n",
      "                             1005018654.5168056],\n",
      "                            [\"Thomas's bicycle\",\n",
      "                             'AtLocation',\n",
      "                             \"Thomas's town\",\n",
      "                             1005105054.5175619]],\n",
      " 'prediction_hand_crafted': None,\n",
      " 'question': [\"Charles's umbrella\", 'AtLocation'],\n",
      " 'semantic_memory_system': [['toothbrush', 'AtLocation', 'suitcase', 2]]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(random_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample has maximum of 4 episodic memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Mary's knife\", 'AtLocation', \"Mary's forest\", 1004845854.5152943],\n",
       " [\"Charles's cat\", 'AtLocation', \"Charles's lap\", 1004932254.5161054],\n",
       " [\"Karen's cow\", 'AtLocation', \"Karen's countryside\", 1005018654.5168056],\n",
       " [\"Thomas's bicycle\", 'AtLocation', \"Thomas's town\", 1005105054.5175619]]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_point[\"episodic_memory_system\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it can have maximum of 4 semantic memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['toothbrush', 'AtLocation', 'suitcase', 2]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_point[\"semantic_memory_system\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell is the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Charles's umbrella\", 'AtLocation']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_point['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your job is to construct a prompt using 1. episodic memory system, 2. semantic memory system, and 3. question.\n",
    "\n",
    "You can make a prompt such as below. Be as creative as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first convert the unix timestamps to something easier ...\n",
    "\n",
    "\n",
    "Just to be sure, let's first order the list of memories by its timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_point[\"episodic_memory_system\"] = sorted(\n",
    "    random_point[\"episodic_memory_system\"], key=lambda x: x[-1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Mary's knife\", 'AtLocation', \"Mary's forest\", 1004845854.5152943],\n",
       " [\"Charles's cat\", 'AtLocation', \"Charles's lap\", 1004932254.5161054],\n",
       " [\"Karen's cow\", 'AtLocation', \"Karen's countryside\", 1005018654.5168056],\n",
       " [\"Thomas's bicycle\", 'AtLocation', \"Thomas's town\", 1005105054.5175619]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_point[\"episodic_memory_system\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it's ordered nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the timestamps more human readable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, mem in enumerate(random_point['episodic_memory_system']):\n",
    "    max_len = len(random_point[\"episodic_memory_system\"])\n",
    "    days = len(random_point['episodic_memory_system']) - idx - 1\n",
    "    if days == 0:\n",
    "        timestamp = \"today\"\n",
    "    else:\n",
    "        timestamp = f\"{days} days ago\"\n",
    "    random_point['episodic_memory_system'][idx][-1] = timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Mary's knife\", 'AtLocation', \"Mary's forest\", '3 days ago'],\n",
       " [\"Charles's cat\", 'AtLocation', \"Charles's lap\", '2 days ago'],\n",
       " [\"Karen's cow\", 'AtLocation', \"Karen's countryside\", '1 days ago'],\n",
       " [\"Thomas's bicycle\", 'AtLocation', \"Thomas's town\", 'today']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_point['episodic_memory_system']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright the episodic memory system looks nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a prompt using 1. episodic memory system, 2. semantic memory system, and 3. question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary's knife was at Mary's forest, 3 days ago. Charles's cat was at Charles's lap, 2 days ago. Karen's cow was at Karen's countryside, 1 days ago. Thomas's bicycle was at Thomas's town, today. 2 toothbrush were found at suitcase. Where is Charles's umbrella?\n"
     ]
    }
   ],
   "source": [
    "prompt = []\n",
    "\n",
    "for mem in random_point[\"episodic_memory_system\"]:\n",
    "    prompt.append(f\"{mem[0]} was at {mem[2]}, {mem[3]}.\")\n",
    "\n",
    "for mem in random_point[\"semantic_memory_system\"]:\n",
    "    prompt.append(f\"{mem[-1]} {mem[0]} were found at {mem[2]}.\")\n",
    "\n",
    "prompt.append(f\"Where is {random_point['question'][0]}?\")\n",
    "\n",
    "prompt = ' '.join(prompt)\n",
    "\n",
    "print(prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you have to use your own creativity to construct prompts.\n",
    "If you can't fit all memories into one prompt due to memory constraint, you should come up \n",
    "with your own strategy to remove some of the memories, if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take this prompt to the huggingface t0pp if it can answer this question properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/bigscience/T0pp?text=Mary%27s+knife+was+at+Mary%27s+forest%2C+3+days+ago.+Charles%27s+cat+was+at+Charles%27s+lap%2C+2+days+ago.+Karen%27s+cow+was+at+Karen%27s+countryside%2C+1+days+ago.+Thomas%27s+bicycle+was+at+Thomas%27s+town%2C+today.+2+toothbrush+were+found+at+suitcase.+Where+is+Charles%27s+umbrella%3F\n",
    "# The answer I get from t0pp is \"at Charles's lap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'closet'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_point['correct_answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the correct answer is \"closet\". In this case, the prediction is false.\n",
    "\n",
    "Global accuracy (i.e. `TP + FP / (TP + FP + TN + FN)`) is good enough for evaluation.\n",
    "We don't have to use weighted F1 score since the classes are pretty much balanced anyways.\n",
    "\n",
    "And also Tae's hand-crafted models are evaluated in global accuracy as well. You want to compare\n",
    "your results to his.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should try your N templates and get N scores on the `val` split. Choose the template\n",
    "that has the best score on the `val` split and then run it on the `test` split to get\n",
    "the test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7808f786822013b9d5984aa54e12ef6bec326a79c76c0a75cd22ab652610adbd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('explicit-memory': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
